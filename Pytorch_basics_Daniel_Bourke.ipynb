{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGtPoNTikezR1nMExeBpfw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ameen2488/Ameen2488/blob/main/Pytorch_basics_Daniel_Bourke.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxMcrwLXEj8o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Data (Preparing and Loading)\n",
        "# Create *known* parameters\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "# Create data\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "\n",
        "X[:10], y[:10]"
      ],
      "metadata": {
        "id": "6ZMv9rqY-Ze-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train/test split\n",
        "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "id": "hpkZRrqpxjYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data=X_train,\n",
        "                     train_labels=y_train,\n",
        "                     test_data=X_test,\n",
        "                     test_labels=y_test,\n",
        "                     predictions=None):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10, 7))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "\n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions in red (predictions were made on the test data)\n",
        "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\": 14});"
      ],
      "metadata": {
        "id": "-49d96c-xolb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Linear Regression model class\n",
        "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
        "                                                dtype=torch.float), # <- PyTorch loves float32 by default\n",
        "                                   requires_grad=True) # <- can we update this value with gradient descent?)\n",
        "\n",
        "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
        "                                            dtype=torch.float), # <- PyTorch loves float32 by default\n",
        "                                requires_grad=True) # <- can we update this value with gradient descent?))\n",
        "\n",
        "    # Forward defines the computation in the model\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
        "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
      ],
      "metadata": {
        "id": "rThlpFxe-3K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create an instance of the model class\n",
        "model_0 = LinearRegressionModel()\n",
        "\n",
        "# Check out the parameters\n",
        "list(model_0.parameters())"
      ],
      "metadata": {
        "id": "JuG1id4434s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Making predictions using 'torch.inference_mode()'\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "y_preds\n"
      ],
      "metadata": {
        "id": "Vn9Y9BiM5brY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "QCKBDvyq-bVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions=y_preds)"
      ],
      "metadata": {
        "id": "vIAFctHV-fId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(model_0.eval().parameters())"
      ],
      "metadata": {
        "id": "0S01hDSJVAPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 3. Train model\n",
        "\n",
        "# Setup a loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# Setup an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.01) # lr = learning rate = possibly the most important hyperparameter you can set"
      ],
      "metadata": {
        "id": "K_oKFyv7_lRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An epoch is one loop through the data...\n",
        "epochs = 10\n",
        "\n",
        "### Training\n",
        "# 0. Loop through the data\n",
        "for epoch in range(epochs): # epoch is a hyperparameter\n",
        " # Set the model to training mode\n",
        "  model_0.train() #\n",
        "\n",
        "  model_0.eval() # Turns off gradient tracking\n",
        "\n",
        " # 1. Forward pass\n",
        "  y_pred = model_0(X_train)\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Perform backpropagation on the loss with respect to the parameters of the model\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizer step (perform gradient descent)\n",
        "  optimizer.step() # by default how the optimizer changes will accumulate through the loop\n"
      ],
      "metadata": {
        "id": "Zjqaa3StSGnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbzwCBSnUme8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}